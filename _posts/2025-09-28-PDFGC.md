# 1. Introduction
*   **Talking Head Synthesis의 중요성**:
    *   사실적인 비디오 아바타를 생성하는 데 필수적인 작업이다.
    *   시각적 더빙, 대화형 라이브 스트리밍, 온라인 회의 등 다양한 애플리케이션에서 활용 가능하다.
*   **기존 연구 동향**:
    *   최근 딥러닝 기술을 활용하여 생생한 Talking Head를 one-shot으로 생성하는 데 큰 진전이 있었다.
    *   주요 방법론은 크게 두 가지로 나뉜다.
        *   **Audio-driven talking head synthesis**: 오디오 신호로부터 정확한 입술 움직임 합성에 중점을 둔다.
        *   **Video-driven face reenactment**: 소스 비디오의 모든 얼굴 움직임을 대상 인물에게 충실하게 전이하는 것을 목표로 하며, 이러한 움직임을 개별 제어 없이 하나의 단위로 처리하는 경향이 있다.
*   **기존 방법론의 한계 및 본 연구의 필요성**:
    *   여러 얼굴 모션에 대한 세분화되고 분리된 제어(fine-grained and disentangled control)가 사실적인 Talking Head를 구현하는 핵심이라고 주장한다. 즉, 입술 움직임, 머리 자세, 눈 움직임, 표정을 각각 별도의 제어 신호로 분리하여 제어할 수 있어야 한다.
    *   이는 분리 표현 학습(disentangled representation learning)이라는 연구 측면에서도 중요하며, 실제 응용 분야에서도 큰 영향을 미친다. 예를 들어, 이미 합성된 Talking Head의 눈 시선을 변경하고 싶을 때, 다른 모든 모션까지 새로 만들 필요 없이 눈 시선만 바꿀 수 있어야 한다.
    *   하지만 이러한 요인들을 분리된 방식으로 제어하는 것은 매우 어렵다.
        *   입술 움직임은 본질적으로 감정과 밀접하게 얽혀 있어, 동일한 음성 내용이라도 감정에 따라 입 모양이 다를 수 있다.
        *   모든 요인을 분리하기 위한 대규모 주석 데이터가 부족하다.
    *   그 결과, 기존 방법들은 특정 요인(예: 눈 시선, 표정)을 수정할 수 없거나, 전체를 한꺼번에 변경하거나, 개별 요인에 대한 정밀한 제어에 어려움을 겪고 있다.
*   **본 논문(PD-FGC)의 해결 방안**:
    *   PD-FGC(Progressive Disentangled Fine-Grained Controllable Talking Head)를 제안한다.
    *   입술 움직임, 머리 자세, 눈 깜빡임/시선, 감정 표현에 대한 분리된 제어가 가능하다.
    *   입술 움직임은 오디오에서, 다른 모션들은 각각 다른 비디오로 개별적으로 구동될 수 있다.
    *   각 모션 요인에 대해 분리된 잠재 표현(latent representation)을 학습하고, 이를 입력으로 받아 Talking Head를 합성하는 이미지 생성기(image generator)를 활용한다.
    *   야생(in-the-wild) 비디오 데이터만을 사용하여 훈련하는 것이 어렵기 때문에, 각 모션의 고유한 속성을 최대한 활용한다.
    *   **진보적 분리 표현 학습 전략(progressive disentangled representation learning strategy)**을 제안한다.
        *   **1단계: Appearance 및 Motion 분리**: 데이터 증강(data augmentation) 및 자기 구동(self-driving)을 통해 외모 정보는 제외하고 모든 모션 정보를 기록하는 통합 모션 특징(unified motion feature)을 먼저 추출한다.
        *   **2단계: 세분화된 Motion 분리**: 통합 모션 특징에서 입술 움직임, 눈 깜빡임/시선, 머리 자세에 대한 개별 모션 표현을 학습한다. 모션별 대조 학습(motion-specific contrastive learning)과 3D 자세 추정기(3D pose estimator)의 도움을 받는다.
        *   **3단계: Expression 분리**: 감정 표현은 다른 모션과 매우 얽혀 있기 때문에, 특징 레벨 디커플링(feature-level decorrelation)을 통해 다른 모션 요인들과 분리하며, 동시에 자기 재구성(self-reconstruction)을 통해 이미지 생성기를 학습하여 의미론적으로 의미 있는 표현(expression)을 학습한다.
*   **주요 기여**:
    1.  정교하게 설계된 진보적 분리 표현 학습 전략을 활용하여 외모, 입술 움직임, 머리 자세, 눈 깜빡임/시선, 감정 표현을 분리하는 새로운 one-shot 세분화 가능 Talking Head 합성 방법을 제안한다.
    2.  원하는 요인 분리(factor disentanglement)를 달성하기 위해 모션별 대조 학습(motion-specific contrastive learning)과 특징 레벨 디커플링(feature-level decorrelation)을 도입한다.
    3.  사전 모델의 제한적인 가이드만으로 비정형 비디오 데이터(unstructured video data)에서 훈련되어, 다양한 얼굴 모션에 대해 다른 구동 신호를 통해 정밀하게 제어할 수 있다. 이는 기존 방법으로는 거의 달성하기 어려운 부분이다.

# 2. Related work

## Audio-driven talking head synthesis

*   **오디오 기반 토킹 헤드 합성의 목표**: 주어진 음성 오디오에 맞춰 입술 움직임이 동기화된 얼굴 이미지를 생성하는 것을 목표로 한다. 초기 연구에서는 주로 입술 영역의 움직임 제어에 초점을 맞추었으며, 다른 얼굴 부분은 변경하지 않고 유지하는 방식이었다.

*   **기존 연구의 확장**:
    *   **다양한 얼굴 속성 제어**: 최근 연구들은 눈 깜빡임(eye blink)이나 머리 자세(head pose)와 같은 추가적인 얼굴 속성까지 제어할 수 있도록 기능을 확장하고 있다.
    *   **감정 표현 도입**: 더욱 생생한 토킹 헤드 생성을 위해 감정 표현(emotional expression)의 변화를 합성 과정에 도입하려는 시도가 있었다.

*   **감정 표현 제어의 어려움**:
    *   **데이터 부족**: 감정 표현을 토킹 헤드 합성에 통합하는 것은 표현력이 풍부한 데이터가 부족하여 매우 어려운 과제이다.
    *   **일반화의 한계**: 일부 방법들(EAMM, MEAD)은 수동으로 수집된 감정 토킹 헤드 데이터셋에 의존하지만, 데이터 범위가 제한적이어서 대규모 시나리오에 잘 일반화되지 못한다.

*   **선행 연구 GC-AVT의 접근 방식**: 최근 연구인 GC-AVT [34]는 'in-the-wild' 데이터(제한되지 않은 실제 환경에서 수집된 데이터)를 활용하여 표현력이 풍부한 토킹 헤드 합성을 시도한다. 이 방법은 입술 움직임과 다른 얼굴 표현을 분리하기 위해 입 영역 데이터 증강(mouth-region data augmentation)을 도입하여 감정 표현에 대한 disentangled control을 달성한다.

*   **본 논문(PD-FGC)의 차별점**:
    *   **특징 레벨 비상관화(feature-level decorrelation)**: GC-AVT와는 다르게 본 논문은 특징 레벨의 비상관화(decorrelation)를 활용하여 입술 움직임과 다른 얼굴 표현이라는 두 요인을 분리한다.
    *   **다양한 요인 개별 제어**: 본 논문의 방법은 입술 움직임(lip motion), 머리 자세(head pose), 눈 응시 및 깜빡임(eye gaze&blink), 그리고 감정 표현(expressions)을 개별적으로 제어할 수 있는 disentangled control을 통해 임의의 토킹 헤드를 합성할 수 있다. 이는 이전 방법들이 모든 요인을 개별적으로 제어하는 데 어려움을 겪었던 한계를 극복하는 핵심적인 기여이다.

## Video-driven face reenactment

*   **워핑 기반 방법 (Warping-based methods)**
    *   **원리:** 소스 프레임(source frame)과 타겟 프레임(target frame) 사이의 워핑 플로우(warping flows)를 예측한다.
    *   **적용:** 예측된 워핑 플로우를 사용하여 타겟 이미지 또는 이미지에서 추출된 특징(features)을 소스 움직임에 맞춰 변형한다.
    *   **예시 연구:** First Order Motion Model for Image Animation, One-shot free-view neural talking-head synthesis for video conferencing 등이 있다.

*   **합성 기반 접근법 (Synthesis-based approaches)**
    *   **원리:** 입력 이미지에서 중간 표현(intermediate representations)을 학습한 다음, 이 표현들을 생성기(generator)로 직접 보내어 이미지를 합성한다.
    *   **중간 표현의 종류:**
        *   **랜드마크(landmarks):** 얼굴의 특정 지점들을 나타내는 정보이다. 예: LI-Net: Large-Pose Identity-Preserving Face Reenactment Network, LandmarkGAN: Synthesizing faces from landmarks, One-shot identity-preserving portrait reenactment
        *   **3D 얼굴 모델 파라미터 또는 메쉬(meshes):** 3D 얼굴 모델의 형상이나 움직임을 제어하는 파라미터 또는 3D 메시 형태의 데이터이다. 예: UniFaceGAN: A Unified Framework for Temporally Consistent Facial Video Editing, Deep video portraits, PIRenderer: Controllable portrait image generation via semantic neural rendering, Face2Face: Real-Time Face Capture and Reenactment of RGB Videos
        *   **이미지에서 추출된 잠재 특징(latent features):** 딥러닝 모델이 이미지에서 학습한 추상적인 특징 표현이다. 예: Finding directions in GAN’s latent space for neural face reenactment, Neural Head Reenactment with Latent Pose Descriptors, Latent Image Animator: Learning to Animate Images via Latent Space Navigation
    *   **최근 동향:** 일부 최근 방법들은 사전 학습된 2D GAN (Analyzing and Improving the Image Quality of StyleGAN)으로부터 얻은 사전 지식(prior knowledge)을 활용하여 얼굴 이미지를 애니메이션화한다.

*   **본 논문의 접근 방식:**
    *   본 논문에서 제안하는 방법은 합성 기반 접근 방식(synthesis-based approach)에 기반을 둔다.
    *   **핵심:** 설계된 진보적인 비분리 표현 학습(progressive disentangled representation learning) 전략을 통해 여러 얼굴 움직임에 대한 비분리 잠재 표현(disentangled latent representations)을 학습한다.
    *   **특이점:** 다른 비디오 기반 접근 방식과 달리, 립 모션(lip motion)은 오디오 신호(audio signals)를 통해 제어한다는 차이점이 있다.

## Disentangled representation learning on the face
얼굴의 Disentangled Representation Learning은 얼굴 이미지나 비디오에서 여러 시각적 요소를 독립적인 잠재 표현으로 분리하여 학습하는 연구 분야이다. 이 연구는 오랫동안 지속되어 왔으며 다양한 접근 방식이 탐구되어 왔다.

*   **비지도 표현 학습(Unsupervised representation learning) 방법:**
    *   초기 연구들 [11, 12, 18, 26, 33, 36, 41, 63]은 InfoGAN [12]과 β-VAE [26]와 같은 비지도 학습(unsupervised learning)에 중점을 두었다.
    *   그러나 이 방법들은 인간의 인지(human perception)와 잘 일치하는 의미 있는 잠재 표현을 보장하지 못한다는 한계 [38]가 있었다. 즉, 모델이 스스로 분리한 요소들이 우리가 직관적으로 이해하는 얼굴의 특정 속성(예: 표정, 포즈)과 정확히 대응되지 않을 수 있다는 의미이다.

*   **사전 학습된 2D GAN 잠재 공간 편집(Latent space editing of pre-trained 2D GAN) 방법:**
    *   최근에는 사전 학습된 2D GAN [31]의 잠재 공간을 편집하는 방법 [49–51, 65, 76, 80]이 주목받았다.
    *   이 방법들은 특정 분류기(classifier)의 도움을 받아 원하는 얼굴 속성에 대한 분리된 제어를 달성한다.
    *   하지만, 이러한 제어 가능성은 주로 선형 분류기(linear classifiers)와 사전 학습된 생성기(generator)의 데이터 분포(data distribution)에 의해 제한되는 경향이 있었다. 복잡하거나 미묘한 제어에는 한계가 있음을 시사한다.

*   **강력한 사전 지식(Prior knowledge) 활용 방법:**
    *   일부 방법 [16, 19, 22, 47, 48, 67]은 3D 얼굴 모델 [45] 또는 표정 모델 [47]과 같은 더 강력한 사전 지식을 활용하여 표현 학습을 안내한다.
    *   이들은 구조화된 데이터(structured data)에 대한 특정 훈련 스키마(training schemes)를 개발하여 원하는 요소 분리(factor disentanglement)를 달성한다. 이는 비지도 학습이나 단순 GAN 편집보다 더 정확하고 의미 있는 분리를 가능하게 하는 접근 방식이다.

*   **본 논문의 접근 방식:**
    *   본 논문은 신중하게 설계된 점진적(progressive) 훈련 스키마를 통해 비디오 데이터에서 분리된 표현 학습을 달성한다.
    *   또한, 특정 사전 모델 [15]을 도입하여 정확한 요소 제어(accurate factor control)를 돕는다. 이는 이전 방법들이 가진 한계(예: 비지도 학습의 의미 부족, GAN 편집의 제어 제한)를 극복하고 다양한 얼굴 요소를 효과적으로 분리하여 현실적인 Talking Head를 생성하는 데 기여한다.

# 3. Method
![alt text](image-3.png)
*   **연구 목표**
    *   주어진 임의의 인물 이미지로부터 Talking Head 비디오를 합성하는 것이 목표이다.
    *   합성된 비디오에서는 립 모션(lip motion), 고개 움직임(head pose), 눈 응시 및 깜빡임(eye gaze&blink), 감정 표현(emotional expression) 등 각 얼굴 움직임을 프레임별로 개별적으로 제어할 수 있다.
    *   립 모션은 오디오 클립에서, 다른 모션들은 각각 다른 비디오로부터 파생되도록 의도한다.

*   **제안하는 방법론: Progressive Disentangled Representation Learning**
    *   **핵심 아이디어:** 제어 가능한 모든 얼굴 움직임과 외모(appearance)를 disentangled latent representations (분리된 잠재 표현)을 통해 표현하고, 이 잠재 표현들을 입력으로 받아 Talking Head를 합성하는 image generator를 학습하는 것이다.
    *   **학습 전략:** 잠재 표현들을 "coarse-to-fine"(거칠게에서 세밀하게) 방식으로 학습하는 Progressive Disentangled Representation Learning 기법을 도입한다. 이 전략은 세 단계로 구성된다.

    1.  **1단계: Appearance 및 Motion Disentanglement (Sec. 3.1)**
        *   **목표:** 외모(appearance)와 얼굴 움직임(facial motions)을 분리하여 '통합 모션 특징(unified motion representation)'을 얻는 것이다.
        *   **역할:** 이 통합 모션 특징은 외모 정보는 배제하고 모든 모션 정보를 기록하며, 이후 세분화된 모션 분리를 위한 강력한 시작점으로 작용한다.

    2.  **2단계: Fine-Grained Motion Disentanglement (Sec. 3.2)**
        *   **목표:** 통합 모션 특징으로부터 립 모션, 눈 응시 및 깜빡임, 고개 움직임 등 세분화된 개별 모션 특징을 분리하는 것이다 (감정 표현은 이 단계에서 제외).
        *   **방법:** 각 모션의 고유한 특성에 기반한 'motion-specific contrastive learning'을 통해 이를 수행한다.

    3.  **3단계: Expression Disentanglement (Sec. 3.3)**
        *   **목표:** 감정 표현을 다른 모션들로부터 분리하고, 동시에 세분화된 제어가 가능한 Talking Head 합성을 위한 image generator를 학습하는 것이다.
        *   **방법:** 'feature-level decorrelation'을 통해 감정 표현을 다른 모션과 분리하며, image generator의 'self-reconstruction'을 통해 의미론적으로 정확한 감정 표현을 학습한다.

## 3.1. Appearance and Motion Disentanglement
이 섹션은 정교하게 제어 가능한 Talking Head를 합성하기 위한 첫 번째 단계인 '외모(Appearance)'와 '움직임(Motion)'을 분리하는 과정에 대해 설명한다.

*   **목표:**
    *   다양한 미세한 움직임 요소를 효과적으로 분리하기 위해, 외모(즉, 신원) 정보는 배제하고 모든 종류의 움직임 정보만을 기록하는 **통합된 움직임 표현** (unified motion representation)을 학습하는 것이 주된 목표이다.
    *   이 통합된 움직임 특징은 이후 단계에서 더욱 세밀한 움직임 요소들을 분리하는 강력한 출발점 역할을 한다.

*   **주요 구성 요소 및 학습 방법:**
    *   **외모 인코더 ($E_{app}$):** 외모를 나타내는 이미지에서 외모 특징을 추출한다.
    *   **움직임 인코더 ($E_{mot}$):** 드라이빙(driving) 프레임에서 움직임 특징을 추출한다.
    *   **생성자 ($G_0$):** $E_{app}$에서 추출한 외모 특징과 $E_{mot}$에서 추출한 움직임 특징을 결합하여 얼굴 이미지를 합성한다.
    *   **Self-driving 및 Reconstruction:** 이 전체 파이프라인은 원본 논문 Neural Head Reenactment with Latent Pose Descriptors의 방법을 따라 학습된다.
    *   **데이터 증강(Data Augmentation):** 움직임 브랜치에 데이터 증강을 적용하여, 움직임 인코더가 외모 변화에는 영향을 받지 않고 오직 움직임 정보 추출에만 집중하도록 강제한다.
    *   **외모 인코더 ($E_{app}$)의 입력 이미지:** 하나의 비디오 시퀀스에서 무작위로 추출된 K개의 프레임 집합으로, 사람의 고유한 아이덴티티(얼굴 특징, 조명, 옷 등 포즈와 무관한 정보)를 추출하는 데 사용된다.
    *   **움직임 인코더($E_{mot}$)의 입력 이미지:** 논문에 따르면, $E_{mot}$는 Neural Head Reenactment with Latent Pose Descriptors에 따라  K개의 프레임과 별개로 선택된 하나의 홀드아웃(hold-out) 프레임 K+1이다. 이 프레임은 포즈는 그대로 유지하면서 스케일, 블러, 샤프닝, 대비 등을 이용하여 아이덴티티 관련 정보를 교란한다.

*   **움직임 재구성 손실($L_{mot}$):**
    *   추출된 움직임 특징의 정확도를 더욱 향상시키기 위해, 원본 논문 Neural Head Reenactment with Latent Pose Descriptors의 학습 손실 위에 **움직임 재구성 손실**을 추가한다.
    *   수식은 다음과 같다.
        $$L_{mot} = ‖φ(I_0) - φ(I_g )‖_2 + ‖ψ(I_0) - ψ(I_g )‖_2$$
    *   **각 항의 의미:**
        *   $L_{mot}$: 움직임 재구성 손실(Motion Reconstruction Loss)이다. 합성된 이미지의 움직임 특징이 실제 이미지의 움직임 특징과 얼마나 잘 일치하는지 측정한다.
        *   $I_0$: 외모 특징과 움직임 특징을 입력으로 받아 생성자 $G_0$에 의해 합성된(synthesized) 이미지이다.
        *   $I_g$: 실제(ground truth) 이미지이다.
        *   $\phi(\cdot)$: EMOCA: Emotion driven monocular face capture and animation에서 참조된 3D 얼굴 재구성 네트워크에 의해 추출된 특징이다. 이는 주로 얼굴의 3D 포즈(pose)와 같은 기하학적 움직임 정보를 나타낸다.
        *   $\psi(\cdot)$: EMOCA: Emotion driven monocular face capture and animation에서 참조된 감정 네트워크에 의해 추출된 특징이다. 이는 주로 얼굴 표정(emotional expression)과 관련된 움직임 정보를 나타낸다.
        *   $‖ \cdot ‖_2$: L2-norm을 나타내며, 두 특징 벡터 간의 유클리드 거리를 계산하여 차이의 크기를 측정한다.
    *   **역할:** 이 손실은 합성된 이미지 $I_0$의 3D 포즈 및 감정 관련 특징이 실제 이미지 $I_g$의 해당 특징에 최대한 가깝도록 강제한다. 이를 통해 $G_0$는 외모는 유지하면서도 드라이빙 프레임의 움직임을 정확하게 재구성하는 법을 학습하게 된다.

## 3.2. Fine-Grained Motion Disentanglement
이 부분은 통일된 모션 특징(unified motion feature)으로부터 입술 움직임 특징(lip motion feature)을 분리하는 방법에 대해 설명한다.

*   **목표**: 이전 단계에서 얻은 통일된 모션 특징에서 입술 움직임, 눈 움직임(시선 및 깜빡임), 머리 자세 특징을 더 세부적으로 추출하는 것이 목표이다. 특히 이 단계에서는 감정 표현(expression)을 분리하지 않는데, 이는 감정 표현이 다른 요소들과 강하게 얽혀 있기 때문이며, 이는 최종 단계(3.3절)에서 다루어진다.

*   **핵심 아이디어**: 각 모션의 고유한 특성을 기반으로 모션별 대비 학습(motion-specific contrastive learning)을 설계하거나, 모션을 잘 설명하는 사전 모델(prior model)의 도움을 받는다.

    ### **Lip motion contrastive learning**:
    *   **분리 원리**: 입술 움직임은 통일된 모션 특징과 해당 음성 오디오 간의 공유 정보를 활용하여 다른 모션과 잘 분리될 수 있다는 점을 이용한다. 이는 이전 연구 [77]에서 입증된 바 있다.
    *   **인코더 도입**:
        *   입술 움직임 인코더 $E_{lip}$와 오디오 인코더 $E_{aud}$가 도입된다.
        *   $E_{mot}$은 이전 단계에서 미리 학습된(pre-trained) 모션 인코더이다.
    *   **특징 추출**:
        *   비디오 프레임 집합 $\{v_i\}$와 해당 오디오 신호 $\{a_i\}$가 주어진다.
        *   입술 움직임 특징 $\{f^v_i\} = \{E_{lip} \circ E_{mot}(v_i)\}$는 $E_{mot}$으로 추출된 통일된 모션 특징에 $E_{lip}$를 적용하여 얻는다.
        *   오디오 특징 $\{f^a_i\} = \{E_{aud}(a_i)\}$는 $E_{aud}$를 통해 얻는다.
    *   **대비 쌍 구성**:
        *   각 샘플링된 오디오 특징 $f^a_i$에 대해, 양성 오디오-비디오 쌍 $(f^a_i, f^v_i)$과 $K$개의 음성 오디오-비디오 쌍 $(f^a_i, f^v_k), k \neq i$를 구성한다.
        *   마찬가지로 비디오 특징 $f^v_i$에 대해서도 동일한 방식으로 양성 및 음성 쌍을 구성한다.
    *   **손실 함수 적용**: InfoNCE 손실 [43]을 적용하여 양성 쌍 간의 유사도를 최대화하고 음성 쌍 간의 유사도를 최소화한다.

    $$ L_{a2v} = -\log\left[ \frac{\exp(S(f^a_i,f^v_i))}{\exp(S(f^a_i,f^v_i))+\sum^K_{k=1} \exp(S(f^a_i,f^v_k))} \right] \quad (2) $$
    $$ L_{v2a} = -\log\left[ \frac{\exp(S(f^v_i,f^a_i))}{\exp(S(f^v_i,f^a_i))+\sum^K_{k=1} \exp(S(f^v_i,f^a_k))} \right] \quad (3) $$
    *   **수식 설명**:
        *   $L_{a2v}$: 오디오를 비디오로 매핑하는 방향의 손실 함수이다.
        *   $L_{v2a}$: 비디오를 오디오로 매핑하는 방향의 손실 함수이다.
        *   $S(\cdot, \cdot)$: 두 특징 벡터 사이의 코사인 유사도(cosine similarity)를 계산하는 함수이다. 코사인 유사도는 두 벡터의 방향이 얼마나 유사한지를 나타낸다.
        *   $f^a_i$: $i$번째 오디오 신호로부터 $E_{aud}$를 통해 추출된 오디오 특징이다.
        *   $f^v_i$: $i$번째 비디오 프레임으로부터 $E_{mot}$과 $E_{lip}$를 통해 추출된 입술 움직임 특징이다.
        *   $f^v_k$: $i$번째 오디오 특징 $f^a_i$와 짝지어진 음성 비디오 특징으로, $i$번째 프레임이 아닌 다른 프레임에서 추출된 입술 움직임 특징이다. $k \neq i$인 경우이다.
        *   $f^a_k$: $i$번째 비디오 특징 $f^v_i$와 짝지어진 음성 오디오 특징으로, $i$번째 오디오가 아닌 다른 오디오에서 추출된 오디오 특징이다. $k \neq i$인 경우이다.
        *   $K$: 음성 샘플의 개수를 나타낸다.
        *   두 손실 함수는 분자의 `positive pair` 유사도를 높이고, 분모의 `positive pair`와 `negative pair` 유사도 합에서 `positive pair` 유사도의 비율을 높이는 방식으로 작동한다. 이는 `positive pair`의 유사도를 최대화하고 `negative pair`의 유사도를 최소화하는 효과를 가져온다.

*   **기여 및 활용**: 이 손실 함수는 $E_{lip}$와 $E_{aud}$에 의해 예측된 입술 움직임 특징이 상응하는 비디오 프레임과 오디오에 대해 서로 가깝도록 보장한다. 오디오 신호는 주로 입술 움직임 정보만 포함하고 있기 때문에, 이 과정은 더 나은 요소 분리(factor disentanglement)에 도움이 된다. 또한, 학습된 오디오 인코더는 오디오 구동 입술 움직임 합성(audio-driven lip motion synthesis)에 활용될 수 있다.

    ### **Eye motion contrastive learning**
    *   **핵심 아이디어**: 눈 움직임(눈 응시 및 깜빡임)은 얼굴의 다른 부위에 미치는 영향이 적은 국소적인 움직임이라는 점에 기반한다.
    *   **합성 이미지 생성**: 두 개의 원본 드라이빙 프레임 v1과 v2가 주어졌을 때, v1의 눈 영역과 v2의 다른 얼굴 영역을 합성하여 새로운 '앵커 프레임' va를 생성한다.
    *   **특징 추출**: 별도의 눈 움직임 인코더 Eeye를 사용하여 v1, v2, va로부터 각각의 눈 움직임 특징인 $f_1, f_2, f_a$를 추출한다.
    *   **대조 쌍 구성**:
        *   긍정 쌍(positive pair)은 $(f_1, f_a)$로 구성된다. v1의 눈과 v2의 나머지 부분을 합성한 va는 v1의 눈 특징을 유지해야 한다는 아이디어에 기반한다.
        *   부정 쌍(negative pair)은 $(f_2, f_a)$로 구성된다. va는 v2의 눈 특징을 가지지 않아야 하므로 이 둘은 부정적으로 연결된다.
    *   **InfoNCE 손실 함수**: 이러한 쌍들을 사용하여 InfoNCE 손실 ($L_{eye}$)을 적용한다. 이 손실은 긍정 쌍의 유사성은 최대화하고 부정 쌍의 유사성은 최소화하여, Eeye가 오직 눈 영역의 움직임에만 집중하고 다른 얼굴 영역의 변화에는 둔감하도록 학습시킨다.

    *   **수식 설명**:
        $$L_{eye} = -\log\left[ \frac{\exp(S(f_1, f_a))}{\exp(S(f_1, f_a)) + \exp(S(f_2, f_a))} \right]$$
        *   $S(\cdot, \cdot)$는 코사인 유사도(cosine similarity)를 계산하는 함수이다. 두 특징 벡터가 얼마나 같은 방향을 향하고 있는지를 나타낸다.
        *   $f_1, f_a, f_2$는 각각 드라이빙 프레임 v1, 앵커 프레임 va, 드라이빙 프레임 v2에서 추출된 눈 움직임 특징이다.
        *   $\exp(\cdot)$는 지수 함수로, 유사도 값을 양수로 만들고 상대적인 크기를 강조한다.
        *   $L_{eye}$는 InfoNCE 손실 함수이다. 이 손실은 $f_1$과 $f_a$ 사이의 유사도를 높이는 동시에 $f_2$와 $f_a$ 사이의 유사도를 낮추는 방식으로 눈 움직임 인코더 Eeye가 학습되도록 유도한다.

*   **머리 자세 학습 (Head pose learning)**
    *   **핵심 아이디어**: 머리 자세(Head pose)는 피치(pitch), 요(yaw), 롤(roll)의 세 가지 오일러 각(Euler angles)과 3D 변환으로 구성된 6차원(6D) 파라미터로 명확하게 정의될 수 있다는 점을 활용한다.
    *   **회귀 학습**: 머리 자세 인코더 Epose는 이러한 6D 파라미터를 직접 회귀(regress)하도록 학습된다.
    *   **3D Face Prior 모델 활용**: 학습 과정에서는 외부의 3D 얼굴 재구성 모델 [15]이 제공하는 ground truth 머리 자세 파라미터($P_{gt}$)를 가이드로 사용한다.
    *   **L1 손실 함수**: Epose가 예측한 머리 자세 파라미터($P_{pred}$)와 ground truth 파라미터($P_{gt}$) 사이의 L1 거리(절대 오차)를 최소화하는 방식으로 학습이 진행된다.

    *   **수식 설명**:
        $$L_{pose} = \left\|P_{pred} - P_{gt}\right\|_1$$
        *   $P_{pred}$는 머리 자세 인코더 Epose에 의해 예측된 머리 자세 파라미터이다.
        *   $P_{gt}$는 외부 3D 얼굴 재구성 모델 [15]로부터 얻은 ground truth(정답) 머리 자세 파라미터이다.
        *   $\left\|\cdot\right\|_1$는 L1 노름(norm)을 나타낸다. 이는 두 벡터의 각 요소별 차이의 절댓값들을 모두 더한 값이다. 즉, 예측된 머리 자세 파라미터와 실제 머리 자세 파라미터 간의 절대 오차를 최소화하는 것을 목표로 한다.
        *   $L_{pose}$는 Epose를 학습하기 위한 손실 함수이다. 이 손실을 최소화함으로써 Epose는 실제 머리 자세와 일치하는 파라미터를 정확하게 예측하도록 학습된다.

## 3.3. Expression Disentanglement
Progressive Disentangled Representation Learning의 세 번째이자 마지막 단계는 표정(Emotional Expression)을 다른 모션 요인들과 분리하는 것이다.

*   **도전 과제**
    *   **다른 모션과의 높은 연관성:** 감정적인 표정은 입 움직임과 같은 다른 얼굴 움직임과 매우 밀접하게 얽혀 있다. 예를 들어, 같은 말을 하더라도 감정에 따라 입 모양이 다를 수 있다.
    *   **정확한 가이드의 부족:** 기존의 표정 추정기(expression estimator)는 종종 표정 외의 다른 모션 정보도 함께 포함하고 있어, 순수한 표정 분리 학습에 정확한 가이드를 제공하기 어렵다.

*   **해결 전략**
    이러한 도전 과제를 해결하기 위해, 이 방법은 "feature-level decorrelation" 전략과 "self-reconstruction"을 통한 보완 학습을 제안한다. 핵심 가설은 추출된 표정 특징이 다른 모션 특징과 독립적이면서도, 이들을 결합했을 때 원본 driving 신호의 모든 얼굴 움직임을 충실하게 재구성할 수 있다면, 그것이 정확한 표정의 잠재 표현이라는 것이다.

    1.  **In-window decorrelation (윈도우 내 상관관계 제거)**
        *   **개념:** 비디오 시퀀스에서 표정 변화는 다른 움직임(예: 입술 움직임, 시선)보다 빈번하지 않다는 관찰에 기반한다.
        *   **작동 방식:** 특정 프레임을 중심으로 시간 윈도우(크기 $K$)를 설정하고, 이 윈도우 내의 프레임들로부터 추출된 표정 특징들의 평균을 계산한다. 이 평균 특징을 중심 프레임의 표정 특징으로 사용함으로써, 윈도우 내에서 평균화되는 동안 다른 모션 정보는 상쇄되어 표정 특징에서 제거되고, 순수한 표정 특징만 남게 된다. 이 평균 특징은 이후 이미지 생성기 $G$로 전송되어 이미지 합성 및 자가 재구성에 사용된다.

    2.  **Lip-motion decorrelation (입술 움직임 상관관계 제거)**
        *   **개념:** 표정 특징과 입술 움직임 특징(오디오 특징) 사이에 독립성을 강제하여 표정 분리를 개선하는 것이다.
        *   **손실 함수:** 다음 수식으로 Lip-motion decorrelation loss $L_{decor}$를 정의한다.

            $$L_{decor} = \frac{1}{D}\sum_{B,D}cor(\bar{F}^e, F^a)^2$$

            *   $\bar{F}^e \in \mathbb{R}^{B \times D}$: 배치 크기 $B$ 내의 평균 표정 특징들로 구성된 행렬이다.
            *   $F^a \in \mathbb{R}^{B \times D}$: 해당 오디오 특징 행렬이다.
            *   $D$: 특징의 차원이다.
            *   $cor(\cdot, \cdot)$: 두 행렬 간의 feature 차원 상관관계(correlation)를 계산하는 함수이다. 이 손실은 표정 특징과 오디오 특징이 서로 상관관계가 없도록 만든다.
        *   **메모리 뱅크:** 상관관계 계산은 높은 정확도를 위해 큰 배치 크기를 요구하지만, 메모리 제한으로 인해 어려운 경우가 많다. 이를 해결하기 위해 표정 특징과 오디오 특징을 위한 두 개의 메모리 뱅크를 유지하며, 이 메모리 뱅크는 항상 $M$개의 최신 특징을 저장하여 $L_{decor}$를 계산하는 데 사용한다. 기울기는 현재 배치 특징을 통해서만 네트워크 가중치를 업데이트한다.

    3.  **Complementary learning via self-reconstruction (자가 재구성을 통한 보완 학습)**
        *   **개념:** 위 두 가지 상관관계 제거 전략이 특징의 독립성을 보장하지만, 추출된 표정 특징 자체의 의미론적 의미는 부족할 수 있다. 이를 위해 이미지 생성기 $G$를 활용하여 표정 특징에 의미론적 의미를 부여한다.
        *   **작동 방식:** 이미지 생성기 $G$는 외모 특징, 다른 모션 특징(입술 움직임, 머리 포즈, 시선/눈 깜빡임)과 함께 표정 특징을 입력받아, driving 프레임을 자가 재구성하도록 학습된다.
            *   이 과정에서 표정 인코더 $E_{exp}$는 다른 모든 모션 특징에 포함되지 않은 (즉, 순수한) 표정 정보를 학습하도록 강제된다.
            *   여러 손실 함수가 $E_{exp}$와 $G$를 학습하는 데 사용된다.
                *   **VGG Loss ($L_{vgg}$):** 합성된 이미지($I_f$)와 Ground Truth 이미지($I_g$) 간의 VGG 특징 맵 차이를 측정하여 이미지의 시각적 유사성을 높인다.

                    $$L_{vgg} =\sum_{i=1}^{N}\|VGG_i(I_f) - VGG_i(I_g)\|_1$$

                    *   $VGG_i(\cdot)$: 미리 학습된 VGG19 [53] 네트워크의 $i$번째 레이어에서 추출된 특징 맵이다.
                    *   $N$: 사용된 VGG 레이어의 총 개수이다.
                    *   $I_f$: 생성된 이미지이다.
                    *   $I_g$: Ground Truth 이미지이다.
                *   **Adversarial Loss 및 Discriminator Feature Matching Loss:** [6]에 따라 적용되어 합성된 이미지의 품질을 개선한다.
                *   **Motion-level consistency loss ($L_{con}$):** 합성된 이미지가 driving 프레임의 모든 얼굴 움직임을 잘 따르도록 보장한다.

                    $$L_{con} = \exp(-S(V_{lip}(I_f), E_{aud}(a_g))) + \|G(I_f) - G(I_g)\|_1 + L_{mot}$$

                    *   $E_{aud}$: 이전 단계에서 학습된 오디오 인코더이다.
                    *   $V_{lip}$: 이미지에서 입술 움직임 특징을 추출하기 위해 미리 학습된 인코더이다.
                    *   $G(I)$: 시선 추정기 [1]이다.
                    *   $S(\cdot, \cdot)$: 코사인 유사도(cosine similarity)이다.
                    *   $I_f$: 합성된 이미지이다.
                    *   $I_g$: Ground Truth 이미지이다.
                    *   $a_g$: Ground Truth 오디오이다.
                    *   $L_{mot}$: Eq. (1)에 정의된 모션 재구성 손실이다.

*   **결론:** 이러한 자가 재구성 과정은 특징 수준의 상관관계 제거 전략과 결합되어 표정 특징을 통합 모션 특징으로부터 성공적으로 분리한다. 또한, 이 단계에서 학습된 이미지 생성기 $G$는 분리된 모든 모션 특징과 외모 특징을 입력으로 받아 세밀하게 제어 가능한 Talking Head 합성을 가능하게 한다.